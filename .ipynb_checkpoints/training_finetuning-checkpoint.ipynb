{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Korean FrameNet ###\n",
      "\t# contact: hahmyg@kaist, hahmyg@gmail.com #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "import os\n",
    "from transformers import *\n",
    "from kaiser.src import utils\n",
    "from kaiser.src import dataio\n",
    "from kaiser.src.modeling import BertForJointShallowSemanticParsing\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if device != \"cpu\":\n",
    "    torch.cuda.set_device(0)\n",
    "# device = torch.device('cpu')\n",
    "# torch.cuda.set_device(device)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(0)   \n",
    "random.seed(0)\n",
    "\n",
    "from torch import autograd\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행시간 측정 함수\n",
    "import time\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60)\n",
    "    \n",
    "    result = '{}hour:{}min:{}sec'.format(t_hour,t_min,t_sec)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(retrain=False, pretrained_dir=False):\n",
    "    if pretrained_dir:\n",
    "        print('original model:', pretrained_dir)\n",
    "    else:\n",
    "        print('original model:', 'BERT-multilingual-base')\n",
    "    print('\\n\\tyour model would be saved at', model_dir)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # load a model first\n",
    "    if retrain:\n",
    "#         model_saved_path = pretrained_dir\n",
    "        model = BertForJointShallowSemanticParsing.from_pretrained(pretrained_dir, \n",
    "                                                                   num_senses = len(bert_io.sense2idx), \n",
    "                                                                   num_args = len(bert_io.bio_arg2idx),\n",
    "                                                                   lufrmap=bert_io.lufrmap, \n",
    "                                                                   frargmap = bert_io.bio_frargmap)\n",
    "    else:\n",
    "#         model_saved_path = PRETRAINED_MODEL\n",
    "        model = BertForJointShallowSemanticParsing.from_pretrained(PRETRAINED_MODEL, \n",
    "                                                                   num_senses = len(bert_io.sense2idx), \n",
    "                                                                   num_args = len(bert_io.bio_arg2idx),\n",
    "                                                                   lufrmap=bert_io.lufrmap, \n",
    "                                                                   frargmap = bert_io.bio_frargmap)\n",
    "#         model.to(device)\n",
    "    \n",
    "    print('retrain:', retrain)\n",
    "    tic()\n",
    "    print('\\n### converting data to BERT input...')\n",
    "    trn_data = bert_io.convert_to_bert_input_JointShallowSemanticParsing(trn)\n",
    "    print('\\t ...is done:', tac())\n",
    "    print('\\t#of instance:', len(trn), len(trn_data))\n",
    "    sampler = RandomSampler(trn)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    # load optimizer\n",
    "#     FULL_FINETUNING = True\n",
    "#     if FULL_FINETUNING:\n",
    "#         param_optimizer = list(model.named_parameters())\n",
    "#         no_decay = ['bias', 'gamma', 'beta']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#              'weight_decay_rate': 0.01},\n",
    "#             {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#              'weight_decay_rate': 0.0}\n",
    "#         ]\n",
    "#     else:\n",
    "#         param_optimizer = list(model.classifier.named_parameters()) \n",
    "#         optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "#     optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "    lr = 1e-3    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "    num_training_steps = len(trn_dataloader) // epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "    \n",
    "    max_grad_norm = 1.0\n",
    "#     global_step = 0\n",
    "#     num_of_epoch = 1\n",
    "    num_of_epoch = 1\n",
    "    accuracy_result = []\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # load a fine-tuned model\n",
    "#         print('epoch:', num_of_epoch)\n",
    "#         print('epoch-1 model:', model_saved_path)\n",
    "#         model = BertForJointShallowSemanticParsing.from_pretrained(model_saved_path, \n",
    "#                                                                    num_senses = len(bert_io.sense2idx), \n",
    "#                                                                    num_args = len(bert_io.bio_arg2idx),\n",
    "#                                                                    lufrmap=bert_io.lufrmap, \n",
    "#                                                                    frargmap = bert_io.bio_frargmap)\n",
    "#         model.to(device)\n",
    "        \n",
    "#         lr = 5e-5\n",
    "#         optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "#         num_training_steps = len(trn_dataloader) // epochs\n",
    "#         scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "        \n",
    "        # TRAIN loop\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            model.train()\n",
    "            # add batch to gpu\n",
    "            torch.cuda.set_device(0)\n",
    "#             torch.cuda.set_device(device)\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_lus, b_input_senses, b_input_args, b_token_type_ids, b_input_masks = batch            \n",
    "#             print(b_token_type_ids[0])\n",
    "            # forward pass\n",
    "#             with autograd.detect_anomaly():\n",
    "            loss = model(b_input_ids, lus=b_input_lus, senses=b_input_senses, args=b_input_args,\n",
    "                     token_type_ids=b_token_type_ids, attention_mask=b_input_masks)\n",
    "            # backward pass\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        \n",
    "        # save your model\n",
    "        model_saved_path = model_dir+str(num_of_epoch)+'/'\n",
    "        print('\\n\\tyour model is saved:', model_saved_path)\n",
    "        if not os.path.exists(model_saved_path):\n",
    "            os.makedirs(model_saved_path)\n",
    "        model.save_pretrained(model_saved_path)\n",
    "        \n",
    "        # load a fine-tuned model\n",
    "#         model = BertForJointShallowSemanticParsing.from_pretrained(model_saved_path, \n",
    "#                                                                    num_senses = len(bert_io.sense2idx), \n",
    "#                                                                    num_args = len(bert_io.bio_arg2idx),\n",
    "#                                                                    lufrmap=bert_io.lufrmap, \n",
    "#                                                                    frargmap = bert_io.bio_frargmap)\n",
    "#         model.to(device)\n",
    "        \n",
    "        num_of_epoch += 1\n",
    "\n",
    "        \n",
    "#         break\n",
    "    print('...training is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl = 'framenet'\n",
    "masking = True\n",
    "MAX_LEN = 256\n",
    "batch_size = 6\n",
    "PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "fnversion = '1.7'\n",
    "language = 'multi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) train En-FN with exemplars model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of instances in trn: 211812\n",
      "# of instances in dev: 2272\n",
      "# of instances in tst: 6714\n",
      "data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]\n",
      "\n",
      "### TRAINING\n",
      "MODEL: framenet\n",
      "LANGUAGE: multi\n",
      "PRETRAINED BERT: bert-base-multilingual-cased\n",
      "training data:\n",
      "\t(en): 211812\n",
      "BATCH_SIZE: 6\n",
      "MAX_LEN: 256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/disk/data/models/framenet/enModel-with-exemplar/'\n",
    "trn, dev, tst = dataio.load_data(srl=srl, language='en')\n",
    "# epochs = 10\n",
    "epochs = 9\n",
    "\n",
    "print('')\n",
    "print('### TRAINING')\n",
    "print('MODEL:', srl)\n",
    "print('LANGUAGE:', language)\n",
    "print('PRETRAINED BERT:', PRETRAINED_MODEL)\n",
    "print('training data:')\n",
    "print('\\t(en):', len(trn))\n",
    "print('BATCH_SIZE:', batch_size)\n",
    "print('MAX_LEN:', MAX_LEN)\n",
    "print('')\n",
    "\n",
    "# bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion, pretrained=PRETRAINED_MODEL)\n",
    "# train()\n",
    "# train(retrain=True, pretrained_dir='/disk/data/models/framenet/enModel-with-exemplar/0/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) fine-tuning by Korean FrameNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### loading Korean FrameNet 1.1 data...\n",
      "\t# of instances in training data: 17838\n",
      "\t# of instances in dev data: 2548\n",
      "\t# of instances in test data: 5097\n",
      "# of instances in trn: 17838\n",
      "# of instances in dev: 2548\n",
      "# of instances in tst: 5097\n",
      "data example: [['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '<tgt>', '순이익이', '</tgt>', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "### TRAINING\n",
      "MODEL: framenet\n",
      "LANGUAGE: multi\n",
      "PRETRAINED BERT: bert-base-multilingual-cased\n",
      "training data:\n",
      "\t(ko): 17838\n",
      "BATCH_SIZE: 6\n",
      "MAX_LEN: 256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# by 100%\n",
    "\n",
    "model_dir = '/disk/data/models/framenet/mulModel-100/'\n",
    "epochs = 20\n",
    "\n",
    "trn, dev, tst = dataio.load_data(srl=srl, language='ko')\n",
    "\n",
    "language = 'multi'\n",
    "\n",
    "print('')\n",
    "print('### TRAINING')\n",
    "print('MODEL:', srl)\n",
    "print('LANGUAGE:', language)\n",
    "print('PRETRAINED BERT:', PRETRAINED_MODEL)\n",
    "print('training data:')\n",
    "print('\\t(ko):', len(trn))\n",
    "print('BATCH_SIZE:', batch_size)\n",
    "print('MAX_LEN:', MAX_LEN)\n",
    "print('')\n",
    "\n",
    "# bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion, pretrained=PRETRAINED_MODEL)\n",
    "# train(retrain=True, pretrained_dir='/disk/data/models/dict_framenet/enModel-with-exemplar/6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### loading Korean FrameNet 1.1 data...\n",
      "\t# of instances in training data: 17838\n",
      "\t# of instances in dev data: 2548\n",
      "\t# of instances in test data: 5097\n",
      "# of instances in trn: 17838\n",
      "# of instances in dev: 2548\n",
      "# of instances in tst: 5097\n",
      "data example: [['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '<tgt>', '순이익이', '</tgt>', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "### TRAINING\n",
      "MODEL: framenet\n",
      "LANGUAGE: multi\n",
      "PRETRAINED BERT: bert-base-multilingual-cased\n",
      "training data:\n",
      "\t(ko): 50\n",
      "BATCH_SIZE: 6\n",
      "MAX_LEN: 256\n",
      "\n",
      "used dictionary:\n",
      "\t /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json\n",
      "\t /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json\n",
      "\t /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
      "original model: /disk/data/models/framenet/enModel-with-exemplar/0/\n",
      "\n",
      "\tyour model would be saved at /disk/data/models/framenet/mulModel-25/\n",
      "retrain: True\n",
      "\n",
      "### converting data to BERT input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ...is done: 0hour:0min:0sec\n",
      "\t#of instance: 50 50\n",
      "epoch: 1\n",
      "epoch-1 model: /disk/data/models/framenet/enModel-with-exemplar/0/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../kaiser/src/utils.py:269: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_logits = sm(masked_logit).view(1,-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4148908919758267\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   2%|▏         | 1/50 [00:12<10:28, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/1/\n",
      "Train loss: 1.3843992749849956\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   4%|▍         | 2/50 [00:24<09:53, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/2/\n",
      "Train loss: 1.3214147090911865\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/3/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   6%|▌         | 3/50 [00:35<09:21, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/3/\n",
      "Train loss: 1.3318035470114813\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/4/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   8%|▊         | 4/50 [00:46<09:01, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/4/\n",
      "Train loss: 1.3758887582355075\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 5/50 [00:57<08:40, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/5/\n",
      "Train loss: 1.317187786102295\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/6/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  12%|█▏        | 6/50 [01:09<08:28, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/6/\n",
      "Train loss: 1.3251810404989455\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/7/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  14%|█▍        | 7/50 [01:20<08:09, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/7/\n",
      "Train loss: 1.3111643857426114\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/8/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  16%|█▌        | 8/50 [01:31<07:54, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/8/\n",
      "Train loss: 1.3505793147616916\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/9/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  18%|█▊        | 9/50 [01:42<07:42, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/9/\n",
      "Train loss: 1.3792431950569153\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/10/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 10/50 [01:53<07:29, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/10/\n",
      "Train loss: 1.3404770559734769\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/11/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  22%|██▏       | 11/50 [02:04<07:16, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/11/\n",
      "Train loss: 1.3467484845055475\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/12/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  24%|██▍       | 12/50 [02:15<07:04, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/12/\n",
      "Train loss: 1.3739228314823575\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/13/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  26%|██▌       | 13/50 [02:26<06:51, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/13/\n",
      "Train loss: 1.4698355860180325\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/14/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  28%|██▊       | 14/50 [02:37<06:40, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/14/\n",
      "Train loss: 1.4548249708281622\n",
      "\n",
      "\tyour model is saved: /disk/data/models/framenet/mulModel-25/15/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 15/50 [02:49<06:30, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16\n",
      "epoch-1 model: /disk/data/models/framenet/mulModel-25/15/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-82261bec9a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mbert_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfnversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPRETRAINED_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/disk/data/models/framenet/enModel-with-exemplar/0/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f91c3eafd9bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(retrain, pretrained_dir)\u001b[0m\n\u001b[1;32m     69\u001b[0m                                                                    \u001b[0mnum_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbio_arg2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                                                    \u001b[0mlufrmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlufrmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                                                                    frargmap = bert_io.bio_frargmap)\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk/kaiser/kaiser/src/modeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, num_senses, num_args, lufrmap, frargmap, masking)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_senses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_senses\u001b[0m \u001b[0;31m# total number of all frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_args\u001b[0m \u001b[0;31m# total number of all frame elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msense_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_senses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertPooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# by 25% (4460)\n",
    "\n",
    "model_dir = '/disk/data/models/framenet/mulModel-25/'\n",
    "epochs = 50\n",
    "\n",
    "trn, dev, tst = dataio.load_data(srl=srl, language='ko')\n",
    "\n",
    "# trn = random.sample(trn, k=4460)\n",
    "trn = random.sample(trn, k=50)\n",
    "language = 'multi'\n",
    "\n",
    "print('')\n",
    "print('### TRAINING')\n",
    "print('MODEL:', srl)\n",
    "print('LANGUAGE:', language)\n",
    "print('PRETRAINED BERT:', PRETRAINED_MODEL)\n",
    "print('training data:')\n",
    "print('\\t(ko):', len(trn))\n",
    "print('BATCH_SIZE:', batch_size)\n",
    "print('MAX_LEN:', MAX_LEN)\n",
    "print('')\n",
    "\n",
    "bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion, pretrained=PRETRAINED_MODEL)\n",
    "train(retrain=True, pretrained_dir='/disk/data/models/framenet/enModel-with-exemplar/0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by 50% (8919)\n",
    "\n",
    "model_dir = '/disk/data/models/framenet/mulModel-50/'\n",
    "epochs = 20\n",
    "\n",
    "trn, dev, tst = dataio.load_data(srl=srl, language='ko')\n",
    "\n",
    "trn = random.sample(trn, k=8919)\n",
    "language = 'multi'\n",
    "\n",
    "print('')\n",
    "print('### TRAINING')\n",
    "print('MODEL:', srl)\n",
    "print('LANGUAGE:', language)\n",
    "print('PRETRAINED BERT:', PRETRAINED_MODEL)\n",
    "print('training data:')\n",
    "print('\\t(ko):', len(trn))\n",
    "print('BATCH_SIZE:', batch_size)\n",
    "print('MAX_LEN:', MAX_LEN)\n",
    "print('')\n",
    "\n",
    "# bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion, pretrained=PRETRAINED_MODEL)\n",
    "# train(retrain=True, pretrained_dir='/disk/data/models/dict_framenet/enModel-with-exemplar/6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by 75% (13378)\n",
    "\n",
    "model_dir = '/disk/data/models/framenet/mulModel-75/'\n",
    "epochs = 20\n",
    "\n",
    "trn, dev, tst = dataio.load_data(srl=srl, language='ko')\n",
    "\n",
    "trn = random.sample(trn, k=13378)\n",
    "language = 'multi'\n",
    "\n",
    "print('')\n",
    "print('### TRAINING')\n",
    "print('MODEL:', srl)\n",
    "print('LANGUAGE:', language)\n",
    "print('PRETRAINED BERT:', PRETRAINED_MODEL)\n",
    "print('training data:')\n",
    "print('\\t(ko):', len(trn))\n",
    "print('BATCH_SIZE:', batch_size)\n",
    "print('MAX_LEN:', MAX_LEN)\n",
    "print('')\n",
    "\n",
    "# bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion, pretrained=PRETRAINED_MODEL)\n",
    "# train(retrain=True, pretrained_dir='/disk/data/models/dict_framenet/enModel-with-exemplar/6/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
