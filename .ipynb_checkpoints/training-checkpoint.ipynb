{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1117 17:34:12.495655 140648635881216 file_utils.py:39] PyTorch version 0.4.1 available.\n",
      "I1117 17:34:12.655414 140648635881216 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Korean FrameNet ###\n",
      "\t# contact: hahmyg@kaist, hahmyg@gmail.com #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "import os\n",
    "from transformers import *\n",
    "from kaiser.src import utils\n",
    "from kaiser.src import dataio\n",
    "from kaiser.src.modeling import BertForJointShallowSemanticParsing\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행시간 측정 함수\n",
    "import time\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60)\n",
    "    \n",
    "    result = '{}hour:{}min:{}sec'.format(t_hour,t_min,t_sec)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('your model would be saved at', model_dir)\n",
    "    \n",
    "    if srl == 'propbank-dp':\n",
    "        model = BertForJointShallowSemanticParsing.from_pretrained(PRETRAINED_MODEL, \n",
    "                                                                   num_senses = len(bert_io.sense2idx), \n",
    "                                                                   num_args = len(bert_io.arg2idx),\n",
    "                                                                   srl=srl,\n",
    "                                                                   masking=False)\n",
    "    else:\n",
    "        model = BertForJointShallowSemanticParsing.from_pretrained(PRETRAINED_MODEL, \n",
    "                                                                   num_senses = len(bert_io.sense2idx), \n",
    "                                                                   num_args = len(bert_io.bio_arg2idx),\n",
    "                                                                   lufrmap=bert_io.lufrmap, \n",
    "                                                                   frargmap = bert_io.bio_frargmap)\n",
    "    model.to(device);\n",
    "    \n",
    "    trn_data = bert_io.convert_to_bert_input_JointShallowSemanticParsing(trn)\n",
    "    sampler = RandomSampler(trn)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    # load optimizer\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "    \n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    accuracy_result = []\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # TRAIN loop\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_lus, b_input_senses, b_input_args, b_input_masks = batch            \n",
    "            # forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, lus=b_input_lus, senses=b_input_senses, args=b_input_args,\n",
    "                         attention_mask=b_input_masks)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "#             break\n",
    "\n",
    "        # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    model_saved_path = model_dir+'epoch-'+str(num_of_epoch)+'-joint.pt'        \n",
    "    torch.save(model, model_saved_path)\n",
    "    num_of_epoch += 1\n",
    "        \n",
    "#         break\n",
    "    print('...training is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of instances in trn: 19306\n",
      "# of instances in dev: 0\n",
      "# of instances in tst: 3778\n",
      "data example: [['한국탁구가', '2000년', '시드니올림픽', '본선에', '남녀복식', '2개조씩을', '<tgt>', '파견할', '</tgt>', '수', '있게', '됐다.'], ['_', '_', '_', '_', '_', '_', '_', 'PRED', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '파견.01', '_', '_', '_', '_'], ['ARG0', 'O', 'O', 'ARG2', 'O', 'ARG1', 'X', 'O', 'X', 'O', 'AUX', 'AUX']]\n",
      "\n",
      "MODEL: propbank-dp\n",
      "LANGUAGE: ko\n"
     ]
    }
   ],
   "source": [
    "# srl = 'propbank-dp'\n",
    "# language = 'ko'\n",
    "# masking = False\n",
    "# model_dir = '/disk/data/models/ko-srl-tgt-1117/'\n",
    "# if language == 'en':\n",
    "#     fnversion = 1.7\n",
    "# #     PRETRAINED_MODEL = \"bert-large-cased\"\n",
    "#     MAX_LEN = 256\n",
    "#     batch_size = 6\n",
    "#     PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "# else:\n",
    "#     fnversion = 1.1\n",
    "#     PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "#     MAX_LEN = 256\n",
    "#     batch_size = 6\n",
    "\n",
    "# epochs = 50\n",
    "\n",
    "# trn, dev, tst = dataio.load_data(srl=srl, language=language)\n",
    "# print('')\n",
    "# print('MODEL:', srl)\n",
    "# print('LANGUAGE:', language)\n",
    "\n",
    "# bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion)\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### loading Korean FrameNet 1.1 data...\n",
      "\t# of instances in training data: 17838\n",
      "\t# of instances in dev data: 2548\n",
      "\t# of instances in test data: 5097\n",
      "# of instances in trn: 17838\n",
      "# of instances in dev: 2548\n",
      "# of instances in tst: 5097\n",
      "data example: [['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '<tgt>', '순이익이', '</tgt>', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "MODEL: framenet\n",
      "LANGUAGE: ko\n"
     ]
    }
   ],
   "source": [
    "# srl = 'framenet'\n",
    "# language = 'ko'\n",
    "# masking = True\n",
    "# model_dir = '/disk/data/models/ko-framenet-tgt-1117/'\n",
    "# if language == 'en':\n",
    "#     fnversion = 1.7\n",
    "# #     PRETRAINED_MODEL = \"bert-large-cased\"\n",
    "#     MAX_LEN = 256\n",
    "#     batch_size = 6\n",
    "#     PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "# else:\n",
    "#     fnversion = 1.1\n",
    "#     PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "#     MAX_LEN = 256\n",
    "#     batch_size = 6\n",
    "\n",
    "# epochs = 50\n",
    "\n",
    "# trn, dev, tst = dataio.load_data(srl=srl, language=language)\n",
    "# print('')\n",
    "# print('MODEL:', srl)\n",
    "# print('LANGUAGE:', language)\n",
    "\n",
    "# bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion)\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of instances in trn: 19391\n",
      "# of instances in dev: 2272\n",
      "# of instances in tst: 6714\n",
      "data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]\n",
      "\n",
      "MODEL: framenet\n",
      "LANGUAGE: en\n"
     ]
    }
   ],
   "source": [
    "# srl = 'framenet'\n",
    "# language = 'en'\n",
    "# masking = True\n",
    "# model_dir = '/disk/data/models/en-framenet-tgt-1117/'\n",
    "# if language == 'en':\n",
    "#     fnversion = 1.7\n",
    "# #     PRETRAINED_MODEL = \"bert-large-cased\"\n",
    "#     MAX_LEN = 256\n",
    "#     batch_size = 6\n",
    "#     PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "# else:\n",
    "#     fnversion = 1.1\n",
    "#     PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "#     MAX_LEN = 256\n",
    "#     batch_size = 6\n",
    "\n",
    "# epochs = 50\n",
    "\n",
    "# trn, dev, tst = dataio.load_data(srl=srl, language=language)\n",
    "# print('')\n",
    "# print('MODEL:', srl)\n",
    "# print('LANGUAGE:', language)\n",
    "\n",
    "# bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion)\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl = 'framenet'\n",
    "language = 'en'\n",
    "masking = True\n",
    "model_dir = '/disk/data/models/en-framenet-tgt-large/'\n",
    "if language == 'en':\n",
    "    fnversion = 1.7\n",
    "    PRETRAINED_MODEL = 'bert-large-cased'\n",
    "#     PRETRAINED_MODEL = 'bert-large-cased-whole-word-masking'\n",
    "    MAX_LEN = 256\n",
    "    batch_size = 1\n",
    "#     PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "else:\n",
    "    fnversion = 1.1\n",
    "    PRETRAINED_MODEL = \"bert-base-multilingual-cased\"\n",
    "    MAX_LEN = 256\n",
    "    batch_size = 6\n",
    "\n",
    "epochs = 16\n",
    "\n",
    "trn, dev, tst = dataio.load_data(srl=srl, language=language)\n",
    "print('')\n",
    "print('MODEL:', srl)\n",
    "print('LANGUAGE:', language)\n",
    "\n",
    "bert_io = utils.for_BERT(mode='train', srl=srl, language=language, masking=masking, fnversion=fnversion, pretrained=PRETRAINED_MODEL)\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
