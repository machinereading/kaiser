Using TensorFlow backend.
### Korean FrameNet ###
	# contact: hahmyg@kaist, hahmyg@gmail.com #

# of instances in trn: 211812
# of instances in dev: 2272
# of instances in tst: 6714
data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(ko): 211812
BATCH_SIZE: 6
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json
original model: BERT-multilingual-base

	your model would be saved at /disk/data/models/framenet/enModel-with-exemplar/
retrain: False

### converting data to BERT input...
	 ...is done: 0hour:6min:22sec
	#of instance: 211812 211812
Epoch:   0%|          | 0/20 [00:00<?, ?it/s]../kaiser/src/utils.py:265: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  pred_logits = sm(masked_logit).view(1,-1)
tensor(0.7066, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8229, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8141, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)
tensor(2.0699, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.0349, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9759, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8932, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.3795, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8633, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4906, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.5897, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8505, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.8492, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.0066, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3510, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.6240, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8974, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9311, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4349, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8597, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.9148, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.0118, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.3105, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7270, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(2.1278, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.2632, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.0313, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.5156, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4358, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.9625, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.3187, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8624, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4723, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8996, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4882, device='cuda:0', grad_fn=<AddBackward0>)

tensor(1.0007, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(1.1530, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5164, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.6624, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.0835, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.2545, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7455, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4462, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.4114, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7699, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3032, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.5440, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(1.0180, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(1.0878, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.0639, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8572, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.2116, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6493, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8909, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.5735, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.4182, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8224, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8110, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.7607, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.9380, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.2703, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4615, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4505, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3436, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.5653, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.9545, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4319, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9390, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.9207, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.5343, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9888, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.4502, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7800, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8099, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.1166, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6597, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8538, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4269, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9122, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.6617, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8308, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4759, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3250, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8986, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.3602, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.1802, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6367, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(1.0675, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.7161, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.9007, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4424, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.8823, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4412, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9789, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9066, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.9871, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.2971, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.9731, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6817, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4351, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7798, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4582, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6041, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4234, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.9072, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.5178, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.3412, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7434, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.6359, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.0623, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6039, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(inf, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4453, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.2982, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4845, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(1.4861, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.9352, device='cuda:0', grad_fn=<AddBackward0>)

tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.7887, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(0.4896, device='cuda:0', grad_fn=<AddBackward0>)

Using TensorFlow backend.
### Korean FrameNet ###
	# contact: hahmyg@kaist, hahmyg@gmail.com #

# of instances in trn: 211812
# of instances in dev: 2272
# of instances in tst: 6714
data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(en): 211812
BATCH_SIZE: 6
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json
Using TensorFlow backend.
### Korean FrameNet ###
	# contact: hahmyg@kaist, hahmyg@gmail.com #

# of instances in trn: 211812
# of instances in dev: 2272
# of instances in tst: 6714
data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(en): 211812
BATCH_SIZE: 9
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json
original model: BERT-multilingual-base

	your model would be saved at /disk/data/models/framenet/enModel-with-exemplar/
retrain: False

### converting data to BERT input...
Using TensorFlow backend.
### Korean FrameNet ###
	# contact: hahmyg@kaist, hahmyg@gmail.com #

# of instances in trn: 211812
# of instances in dev: 2272
# of instances in tst: 6714
data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(en): 211812
BATCH_SIZE: 9
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json
original model: BERT-multilingual-base

	your model would be saved at /disk/data/models/framenet/enModel-with-exemplar/
retrain: False

### converting data to BERT input...
	 ...is done: 0hour:6min:16sec
	#of instance: 211812 211812
Epoch:   0%|          | 0/20 [00:00<?, ?it/s]../kaiser/src/utils.py:265: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  pred_logits = sm(masked_logit).view(1,-1)
Train loss: nan

	your model is saved: /disk/data/models/framenet/enModel-with-exemplar/0/
Epoch:   5%|▌         | 1/20 [2:58:16<56:27:15, 10696.63s/it]Traceback (most recent call last):
  File "training_finetuning.py", line 221, in <module>
    train()
  File "training_finetuning.py", line 147, in train
    token_type_ids=b_token_type_ids, attention_mask=b_input_masks)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "../kaiser/src/modeling.py", line 63, in forward
    lufr_masks = utils.get_masks(lus, self.lufrmap, num_label=self.num_senses, masking=self.masking).to(device)        
  File "../kaiser/src/utils.py", line 235, in get_masks
    indx = str(idx).split('[')[-1].split(']')[0]
  File "/usr/local/lib/python3.6/dist-packages/torch/tensor.py", line 130, in __repr__
    return torch._tensor_str._str(self)
  File "/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py", line 311, in _str
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py", line 209, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py", line 83, in __init__
    value_str = '{}'.format(value)
  File "/usr/local/lib/python3.6/dist-packages/torch/tensor.py", line 377, in __format__
    return self.item().__format__(format_spec)
RuntimeError: CUDA error: misaligned address
Epoch:   5%|▌         | 1/20 [4:22:36<83:09:38, 15756.74s/it]
Using TensorFlow backend.
Traceback (most recent call last):
  File "training_finetuning.py", line 13, in <module>
    from kaiser.src import utils
  File "../kaiser/src/utils.py", line 19, in <module>
    torch.cuda.set_device(device)
  File "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py", line 298, in set_device
    device = _get_device_index(device)
  File "/usr/local/lib/python3.6/dist-packages/torch/cuda/_utils.py", line 24, in _get_device_index
    raise ValueError('Expected a cuda device, but got: {}'.format(device))
ValueError: Expected a cuda device, but got: cpu
Using TensorFlow backend.
Traceback (most recent call last):
  File "training_finetuning.py", line 14, in <module>
    from kaiser.src import dataio
  File "../kaiser/src/dataio.py", line 11, in <module>
    from frameBERT.koreanframenet import koreanframenet
ModuleNotFoundError: No module named 'frameBERT.koreanframenet'
Using TensorFlow backend.
### Korean FrameNet ###
	# contact: hahmyg@kaist, hahmyg@gmail.com #

Traceback (most recent call last):
  File "training_finetuning.py", line 15, in <module>
    from kaiser.src.modeling import BertForJointShallowSemanticParsing
  File "../kaiser/src/modeling.py", line 64
    else:
       ^
SyntaxError: invalid syntax
Using TensorFlow backend.
### Korean FrameNet ###
	# contact: hahmyg@kaist, hahmyg@gmail.com #

# of instances in trn: 211812
# of instances in dev: 2272
# of instances in tst: 6714
data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(en): 211812
BATCH_SIZE: 9
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json

### loading Korean FrameNet 1.1 data...
	# of instances in training data: 17838
	# of instances in dev data: 2548
	# of instances in test data: 5097
# of instances in trn: 17838
# of instances in dev: 2548
# of instances in tst: 5097
data example: [['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '<tgt>', '순이익이', '</tgt>', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(ko): 4460
BATCH_SIZE: 9
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json
original model: /disk/data/models/framenet/enModel-with-exemplar/6/

	your model would be saved at /disk/data/models/framenet/mulModel-25/
Traceback (most recent call last):
  File "training_finetuning.py", line 267, in <module>
    train(retrain=True, pretrained_dir='/disk/data/models/framenet/enModel-with-exemplar/6/')
  File "training_finetuning.py", line 94, in train
    frargmap = bert_io.bio_frargmap)
  File "/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py", line 345, in from_pretrained
    **kwargs
  File "/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py", line 163, in from_pretrained
    config = cls.from_json_file(resolved_config_file)
  File "/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py", line 196, in from_json_file
    return cls.from_dict(json.loads(text))
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
Using TensorFlow backend.
### Korean FrameNet ###
	# contact: hahmyg@kaist, hahmyg@gmail.com #

# of instances in trn: 211812
# of instances in dev: 2272
# of instances in tst: 6714
data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(en): 211812
BATCH_SIZE: 9
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json

### loading Korean FrameNet 1.1 data...
	# of instances in training data: 17838
	# of instances in dev data: 2548
	# of instances in test data: 5097
# of instances in trn: 17838
# of instances in dev: 2548
# of instances in tst: 5097
data example: [['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '<tgt>', '순이익이', '</tgt>', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(ko): 4460
BATCH_SIZE: 9
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json
original model: /disk/data/models/dict_framenet/enModel-with-exemplar/6/

	your model would be saved at /disk/data/models/framenet/mulModel-25/
retrain: True

### converting data to BERT input...
	 ...is done: 0hour:0min:6sec
	#of instance: 4460 4460
Epoch:   0%|                                                                                       | 0/50 [00:00<?, ?it/s]Train loss: 2.476619124172195

	your model is saved: /disk/data/models/framenet/mulModel-25/0/
Epoch:   2%|█▍                                                                        | 1/50 [50:27<41:12:42, 3027.80s/it]Train loss: 1.5683065928638942

	your model is saved: /disk/data/models/framenet/mulModel-25/1/
Epoch:   4%|██▉                                                                     | 2/50 [1:40:31<40:16:29, 3020.62s/it]Train loss: 1.0779362106395345

	your model is saved: /disk/data/models/framenet/mulModel-25/2/
Epoch:   6%|████▎                                                                   | 3/50 [2:29:53<39:12:27, 3003.13s/it]Train loss: 0.7446353957357426

	your model is saved: /disk/data/models/framenet/mulModel-25/3/
Epoch:   8%|█████▊                                                                  | 4/50 [3:20:13<38:26:15, 3008.17s/it]Train loss: 0.51732557293989

	your model is saved: /disk/data/models/framenet/mulModel-25/4/
Epoch:  10%|███████▏                                                                | 5/50 [4:11:07<37:46:21, 3021.81s/it]Train loss: 0.3604540412179044

	your model is saved: /disk/data/models/framenet/mulModel-25/5/
Epoch:  12%|████████▋                                                               | 6/50 [5:01:49<37:00:31, 3027.99s/it]Train loss: 0.26673754410547834

	your model is saved: /disk/data/models/framenet/mulModel-25/6/
Epoch:  14%|██████████                                                              | 7/50 [5:51:46<36:03:16, 3018.53s/it]Train loss: 0.20237754046120832

	your model is saved: /disk/data/models/framenet/mulModel-25/7/
Epoch:  16%|███████████▌                                                            | 8/50 [6:41:28<35:05:23, 3007.70s/it]Train loss: 0.15194393463823344

	your model is saved: /disk/data/models/framenet/mulModel-25/8/
Epoch:  18%|████████████▉                                                           | 9/50 [7:31:32<34:14:27, 3006.54s/it]Train loss: 0.12504069142854743

	your model is saved: /disk/data/models/framenet/mulModel-25/9/
Epoch:  20%|██████████████▏                                                        | 10/50 [8:22:45<33:37:34, 3026.36s/it]Train loss: 0.10445116461825467

	your model is saved: /disk/data/models/framenet/mulModel-25/10/
Epoch:  22%|███████████████▌                                                       | 11/50 [9:13:55<32:55:46, 3039.66s/it]Train loss: 0.08299317207532905

	your model is saved: /disk/data/models/framenet/mulModel-25/11/
Epoch:  24%|████████████████▊                                                     | 12/50 [10:03:48<31:56:07, 3025.47s/it]Train loss: 0.0687588598699339

	your model is saved: /disk/data/models/framenet/mulModel-25/12/
Epoch:  26%|██████████████████▏                                                   | 13/50 [10:53:11<30:54:05, 3006.63s/it]Train loss: 0.05766214516704842

	your model is saved: /disk/data/models/framenet/mulModel-25/13/
Epoch:  28%|███████████████████▌                                                  | 14/50 [11:43:20<30:04:32, 3007.56s/it]Train loss: 0.05203441231306868

	your model is saved: /disk/data/models/framenet/mulModel-25/14/
Epoch:  30%|█████████████████████                                                 | 15/50 [12:33:26<29:14:06, 3007.06s/it]Train loss: 0.04139346158227718

	your model is saved: /disk/data/models/framenet/mulModel-25/15/
Epoch:  32%|██████████████████████▍                                               | 16/50 [13:23:32<28:23:52, 3006.84s/it]Train loss: 0.03497769403216925

	your model is saved: /disk/data/models/framenet/mulModel-25/16/
Epoch:  34%|███████████████████████▊                                              | 17/50 [14:12:48<27:25:22, 2991.60s/it]Train loss: 0.02990566625826872

	your model is saved: /disk/data/models/framenet/mulModel-25/17/
Epoch:  36%|█████████████████████████▏                                            | 18/50 [15:02:52<26:37:23, 2995.12s/it]Train loss: 0.027553824865120282

	your model is saved: /disk/data/models/framenet/mulModel-25/18/
Epoch:  38%|██████████████████████████▌                                           | 19/50 [15:52:58<25:49:11, 2998.43s/it]Train loss: 0.027544036498687592

	your model is saved: /disk/data/models/framenet/mulModel-25/19/
Epoch:  40%|████████████████████████████                                          | 20/50 [16:43:23<25:03:14, 3006.49s/it]Train loss: 0.022239110700719458

	your model is saved: /disk/data/models/framenet/mulModel-25/20/
Epoch:  42%|█████████████████████████████▍                                        | 21/50 [17:33:56<24:17:00, 3014.49s/it]Train loss: 0.01889894097926557

	your model is saved: /disk/data/models/framenet/mulModel-25/21/
Epoch:  44%|██████████████████████████████▊                                       | 22/50 [18:24:44<23:31:20, 3024.32s/it]Train loss: 0.019610624699071783

	your model is saved: /disk/data/models/framenet/mulModel-25/22/
Epoch:  46%|████████████████████████████████▏                                     | 23/50 [19:14:26<22:35:17, 3011.77s/it]Train loss: 0.02139672115921595

	your model is saved: /disk/data/models/framenet/mulModel-25/23/
Epoch:  48%|█████████████████████████████████▌                                    | 24/50 [20:05:25<21:51:10, 3025.79s/it]Train loss: 0.017328481663547966

	your model is saved: /disk/data/models/framenet/mulModel-25/24/
Epoch:  50%|███████████████████████████████████                                   | 25/50 [20:55:53<21:01:04, 3026.56s/it]Train loss: 0.016675312144035424

	your model is saved: /disk/data/models/framenet/mulModel-25/25/
Epoch:  52%|████████████████████████████████████▍                                 | 26/50 [21:46:32<20:12:08, 3030.34s/it]Train loss: 0.016797696926944133

	your model is saved: /disk/data/models/framenet/mulModel-25/26/
Epoch:  54%|█████████████████████████████████████▊                                | 27/50 [22:37:07<19:22:09, 3031.72s/it]Train loss: 0.0115203706311384

	your model is saved: /disk/data/models/framenet/mulModel-25/27/
Epoch:  56%|███████████████████████████████████████▏                              | 28/50 [23:26:17<18:22:35, 3007.07s/it]Train loss: 0.01797163502399804

	your model is saved: /disk/data/models/framenet/mulModel-25/28/
Epoch:  58%|████████████████████████████████████████▌                             | 29/50 [24:15:44<17:28:17, 2995.14s/it]Train loss: 0.014670681554566873

	your model is saved: /disk/data/models/framenet/mulModel-25/29/
Epoch:  60%|██████████████████████████████████████████                            | 30/50 [25:05:42<16:38:38, 2995.93s/it]Train loss: 0.009829869684542627

	your model is saved: /disk/data/models/framenet/mulModel-25/30/
Epoch:  62%|███████████████████████████████████████████▍                          | 31/50 [25:55:28<15:47:50, 2993.17s/it]Train loss: 0.01954805426395333

	your model is saved: /disk/data/models/framenet/mulModel-25/31/
Epoch:  64%|████████████████████████████████████████████▊                         | 32/50 [26:45:38<14:59:27, 2998.19s/it]Train loss: 0.01247676801340863

	your model is saved: /disk/data/models/framenet/mulModel-25/32/
Epoch:  66%|██████████████████████████████████████████████▏                       | 33/50 [27:35:07<14:07:00, 2989.45s/it]Train loss: 0.011466254383131239

	your model is saved: /disk/data/models/framenet/mulModel-25/33/
Epoch:  68%|███████████████████████████████████████████████▌                      | 34/50 [28:25:30<13:19:51, 2999.45s/it]Train loss: 0.014431291999039052

	your model is saved: /disk/data/models/framenet/mulModel-25/34/
Epoch:  70%|█████████████████████████████████████████████████                     | 35/50 [29:15:50<12:31:22, 3005.48s/it]Train loss: 0.011386996257369315

	your model is saved: /disk/data/models/framenet/mulModel-25/35/
Epoch:  72%|██████████████████████████████████████████████████▍                   | 36/50 [30:06:22<11:43:10, 3013.62s/it]Train loss: 0.01369377309682081

	your model is saved: /disk/data/models/framenet/mulModel-25/36/
Epoch:  74%|███████████████████████████████████████████████████▊                  | 37/50 [30:55:24<10:48:17, 2992.14s/it]Train loss: 0.014866524059148251

	your model is saved: /disk/data/models/framenet/mulModel-25/37/
Epoch:  76%|█████████████████████████████████████████████████████▏                | 38/50 [31:46:21<10:02:17, 3011.49s/it]Train loss: 0.011861465526958308

	your model is saved: /disk/data/models/framenet/mulModel-25/38/
Epoch:  78%|███████████████████████████████████████████████████████▍               | 39/50 [32:37:23<9:14:51, 3026.48s/it]Train loss: 0.009657536549551173

	your model is saved: /disk/data/models/framenet/mulModel-25/39/
Epoch:  80%|████████████████████████████████████████████████████████▊              | 40/50 [33:27:43<8:24:06, 3024.67s/it]Train loss: 0.013136086928610777

	your model is saved: /disk/data/models/framenet/mulModel-25/40/
Epoch:  82%|██████████████████████████████████████████████████████████▏            | 41/50 [34:16:47<7:30:03, 3000.36s/it]Train loss: 0.009304638859424402

	your model is saved: /disk/data/models/framenet/mulModel-25/41/
Epoch:  84%|███████████████████████████████████████████████████████████▋           | 42/50 [35:06:50<6:40:09, 3001.14s/it]Train loss: 0.011011719653080006

	your model is saved: /disk/data/models/framenet/mulModel-25/42/
Epoch:  86%|█████████████████████████████████████████████████████████████          | 43/50 [35:56:54<5:50:15, 3002.22s/it]Train loss: 0.011051307831577662

	your model is saved: /disk/data/models/framenet/mulModel-25/43/
Epoch:  88%|██████████████████████████████████████████████████████████████▍        | 44/50 [36:47:09<5:00:36, 3006.06s/it]Train loss: 0.012685573665528922

	your model is saved: /disk/data/models/framenet/mulModel-25/44/
Epoch:  90%|███████████████████████████████████████████████████████████████▉       | 45/50 [37:37:25<4:10:44, 3008.92s/it]Train loss: 0.00979125329704109

	your model is saved: /disk/data/models/framenet/mulModel-25/45/
Epoch:  92%|█████████████████████████████████████████████████████████████████▎     | 46/50 [38:26:44<3:19:36, 2994.03s/it]Train loss: 0.011591012316550346

	your model is saved: /disk/data/models/framenet/mulModel-25/46/
Epoch:  94%|██████████████████████████████████████████████████████████████████▋    | 47/50 [39:16:44<2:29:47, 2995.85s/it]Train loss: 0.0093497706959551

	your model is saved: /disk/data/models/framenet/mulModel-25/47/
Epoch:  96%|████████████████████████████████████████████████████████████████████▏  | 48/50 [40:06:33<1:39:47, 2993.62s/it]Train loss: 0.008499499878927319

	your model is saved: /disk/data/models/framenet/mulModel-25/48/
Epoch:  98%|███████████████████████████████████████████████████████████████████████▌ | 49/50 [40:57:59<50:21, 3021.38s/it]Train loss: 0.013301563873849977

	your model is saved: /disk/data/models/framenet/mulModel-25/49/
Epoch: 100%|█████████████████████████████████████████████████████████████████████████| 50/50 [41:48:40<00:00, 3027.36s/it]Epoch: 100%|█████████████████████████████████████████████████████████████████████████| 50/50 [41:48:40<00:00, 3010.41s/it]
...training is done

### loading Korean FrameNet 1.1 data...
	# of instances in training data: 17838
	# of instances in dev data: 2548
	# of instances in test data: 5097
# of instances in trn: 17838
# of instances in dev: 2548
# of instances in tst: 5097
data example: [['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '<tgt>', '순이익이', '</tgt>', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]

### TRAINING
MODEL: framenet
LANGUAGE: multi
PRETRAINED BERT: bert-base-multilingual-cased
training data:
	(ko): 8919
BATCH_SIZE: 9
MAX_LEN: 256

used dictionary:
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lu2idx.json
	 /disk/kaiser/kaiser/src/../koreanframenet/resource/info/mul_lufrmap.json
original model: /disk/data/models/dict_framenet/enModel-with-exemplar/6/

	your model would be saved at /disk/data/models/framenet/mulModel-50/
retrain: True

### converting data to BERT input...
	 ...is done: 0hour:0min:13sec
	#of instance: 8919 8919
Epoch:   0%|                                                                                       | 0/50 [00:00<?, ?it/s]Train loss: 2.2151529652439623

	your model is saved: /disk/data/models/framenet/mulModel-50/0/
Epoch:   2%|█▍                                                                      | 1/50 [1:41:00<82:29:10, 6060.21s/it]Train loss: 1.3724956613346018

	your model is saved: /disk/data/models/framenet/mulModel-50/1/
Epoch:   4%|██▉                                                                     | 2/50 [3:22:24<80:54:03, 6067.58s/it]Train loss: 0.9309451150304735

	your model is saved: /disk/data/models/framenet/mulModel-50/2/
Epoch:   6%|████▎                                                                   | 3/50 [5:01:26<78:43:12, 6029.62s/it]Train loss: 0.6548288569049806

	your model is saved: /disk/data/models/framenet/mulModel-50/3/
Epoch:   8%|█████▊                                                                  | 4/50 [6:39:41<76:31:50, 5989.35s/it]Train loss: 0.473947200307662

	your model is saved: /disk/data/models/framenet/mulModel-50/4/
Epoch:  10%|███████▏                                                                | 5/50 [8:19:19<74:49:29, 5985.99s/it]Train loss: 0.3374050007540992

	your model is saved: /disk/data/models/framenet/mulModel-50/5/
Epoch:  12%|████████▋                                                               | 6/50 [9:59:20<73:13:05, 5990.59s/it]Train loss: 0.2591894057877249

	your model is saved: /disk/data/models/framenet/mulModel-50/6/
Epoch:  14%|█████████▉                                                             | 7/50 [11:37:57<71:17:17, 5968.31s/it]